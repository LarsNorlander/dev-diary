# Manning's [Deep Learning in a Box](https://www.manning.com/bundle-hub/deep-learning)



## October 25, 2018

### 11:00 PM

Today, I am committing again to the #100DaysOfCode challenge. This time, my goal is to go through all of Manning's learning materials on [Deep Learning](https://www.manning.com/bundle-hub/deep-learning). And yes, I actually bought the first bundle. I'm intentionally twisting my arm into studying.

## October 26, 2018

### 9:58 PM

Got to read a bit more about machine learning and how computers learn. I'm getting a little tired now, so I might as well continue tomorrow. No point in forcing myself to study if I can't absorb what I'm reading.

## October 27, 2018

### 11:16 PM

So, didn't get the time to study today coz I went to IKEA to get some furniture. Took a while assembling stuff (and quite the cut on my finger) but now, my desk is so clean and clutter free. I guess that would help a bit to concentrate on studying. I'll definitely put a bunch of effort tomorrow. (Assuming nothing comes up.)

## October 28, 2018

### 8:13 PM

Got to read up and finish the second chapter of Grokking Deep Learning. Now I properly understand the difference between supervised and unsupervised learning. Not only that, I also found out that there's Parametric and Non-Parametric learning. I look forward especially to Chapter 3, where things will start to become really hands on.

## October 29, 2018

### 8:52 PM

Got to read up about half of chapter three from Grokking Deep Learning. I was surprised to see how tiny a neural network could be. I created a few simple neural networks. One of them took one input and produced one output, and one took several inputs and produced one input. Tomorrow I'll continue on with the rest of the chapter and hopefully finish it.

## October 31, 2018

### 8:13 PM

Got to finish up chapter three of Grokking Deep Learning and now I could say that I finally understand the structure of a neural network. It's surprisingly simpler than I thought.

## November 1, 2018

### 8:06 PM

Read into chapter four and I'm starting to see how learning works. So for starters, there's hot and cold learning where you try out a value that's a bit higher and a bit lower to see which one creates the smaller error. Although as I'm about to read, it seems there is a better way to do this.

## November 2, 2018

### 10:17 PM

Read a bit more on learning and got to read up a bit on gradient decent. I'm a little too tired so this will do for now. I'll try watching videos instead and see how much that would help.

## November 3, 2018

### 8:50 PM

I got to finish up chapter 4 of Grokking Deep Learning. Now I see the mathematical connection Deep Learning has. Although honestly, a lot of things flew threw my head. The ending of the chapter now asks that I write the machine learning code from memory... Which I can't do at the moment. So my next focus for tomorrow will be understanding the code enough to be able to do just that.

## November 4, 2018

### 9:15 PM

I went though chapter 4 of Grokking Deep Learning again today and this time, I actually got it. And, I got to recall (most of) the code from the chapter. Next stop, chapter 5! "Learning Multiple Weights at a Time: Generalizing Gradient Descent"

## November 5, 2018

### 7:31 PM

Today was a bit of a long day, so I don't think I'll learn much if I force myself to read today. I'll probably start chapter 5 tomorrow.

## November 6, 2018

### 9:06 PM

Took a break from the book today and watched a few videos by 1Blue3Brown on neural networks. For some reason, watching something and learning visually helps quite well.

## November 8, 2018

### 8:24 PM

I started reading up a bit on chapter 5, which is training with multiple inputs. I find it amusing that the process of adjusting the weight is basically just repeated on all inputs. But I'm only halfway there so there's more to see.

## November 9, 2018

### 6:14 PM

So! I have a new pair of glasses and reading just makes me dizzy at this point. I'm hoping that in a few days, I could easily go back to this and continue on. Maybe if I'm better adjusted tomorrow, I'll pick up the pace tomorrow.

## November 12, 2018

### 8:15 PM

Back at it again! And this time, I finished up chapter 5 of Grokking Deep Learning. It's amusing to see how flexible gradient descent is when it comes to learning. I was quite surprised to see that learning to predict using multiple inputs and outputs was rather simpler than I initially imaged.